{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BsP8X727kQMe"
   },
   "source": [
    "# Midterm. Обучение нейронных сетей на PyTorch.\n",
    "\n",
    "\n",
    "Вам необходимо заполнить пропуски в ноутбуке. Кое-где вас просят сделать выводы о проделанной работе. Постарайтесь ответить на вопросы обдуманно и развёрнуто. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-2rBvEkkQMj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHc7UePMkQMp"
   },
   "source": [
    "# Дисклеймер про CrossEntropyLoss и NLLLoss\n",
    "\n",
    "Обычно в PyTorch не нужно делать Softmax как последний слой модели. \n",
    "\n",
    "* Если Вы используете NLLLoss, то ему на вход надо давать лог вероятности, то есть выход слоя LogSoftmax. (Просто результат софтмакса, к которому применен логарифм)\n",
    "* Если Вы используете CrossEntropyLoss, то применение LogSoftmax уже включено внутрь лосса, поэтому ему на вход надо подавать просто выход обычного линейного слоя без активации. По сути CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "\n",
    "Зачем такие сложности, чтобы посчитать обычную кросс энтропию, которую мы использовали как лосс еще в логистической регрессии? Дело в том, что нам в любом случае придется взять логарифм от результатов софтмакса, а если делать это одной функцией, то можно сделать более устойчивую реализацию, которая даст меньшую вычислительную погрешность. \n",
    "\n",
    "Таким образом, если у вас в конце сети, решающей задачу классификации, стоит просто линейный слой без активации, то вам нужно использовать CrossEntropy. В этой домашке везде используется лосс CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rM9IY0YkQMq"
   },
   "source": [
    "# Задание 1. Создайте генератор батчей. \n",
    "\n",
    "В этот раз мы хотим сделать генератор, который будет максимально похож на то, что используется в реальном обучении. \n",
    "\n",
    "С помощью numpy вам нужно перемешать исходную выборку и выбирать из нее батчи размером batch_size, если размер выборки не делился на размер батча, то последний батч должен иметь размер меньше batch_size и состоять просто из всех оставшихся объектов. Возвращать нужно в формате (X_batch, y_batch). Необходимо написать именно генератор, то есть вместо return использовать yield. \n",
    "\n",
    "Хорошая статья про генераторы: https://habr.com/ru/post/132554/\n",
    "\n",
    "\n",
    "**Ответ на задание - код**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttf6PZuVkQMr"
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    np.random.seed(42)\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # YOUR CODE\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "\n",
    "        batch_idx = indices[start:end]\n",
    "\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RvSIsl-c5lW"
   },
   "source": [
    "Попробуем потестировать наш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_snYtUUcpDy"
   },
   "outputs": [],
   "source": [
    "from inspect import isgeneratorfunction\n",
    "assert isgeneratorfunction(batch_generator), \"batch_generator должен быть генератором! В условии есть ссылка на доки\"\n",
    "\n",
    "X = np.array([\n",
    "              [1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]\n",
    "])\n",
    "y = np.array([\n",
    "              1, 2, 3\n",
    "])\n",
    "\n",
    "# Проверим shape первого батча\n",
    "iterator = batch_generator(X, y, 2)\n",
    "X_batch, y_batch = next(iterator)\n",
    "assert X_batch.shape == (2, 3), y_batch.shape == (2,)\n",
    "assert np.allclose(X_batch, X[:2]), np.allclose(y_batch, y[:2])\n",
    "\n",
    "# Проверим shape последнего батча (их всего два)\n",
    "X_batch, y_batch = next(iterator)\n",
    "assert X_batch.shape == (1, 3), y_batch.shape == (1,)\n",
    "assert np.allclose(X_batch, X[2:]), np.allclose(y_batch, y[2:])\n",
    "\n",
    "# Проверим, что итерации закончились\n",
    "iter_ended = False\n",
    "try:\n",
    "    next(iterator)\n",
    "except StopIteration:\n",
    "    iter_ended = True\n",
    "assert iter_ended\n",
    "\n",
    "# Еще раз проверим то, сколько батчей создает итератор\n",
    "X = np.random.randint(0, 100, size=(1000, 100))\n",
    "y = np.random.randint(-1, 1, size=(1000, 1))\n",
    "num_iter = 0\n",
    "for _ in batch_generator(X, y, 3):\n",
    "    num_iter += 1\n",
    "assert num_iter == (1000 // 3 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJ9_3VfrkQMv"
   },
   "source": [
    "# Задание 2. Обучите модель для классификации звезд\n",
    "\n",
    "Загрузите датасет из файла sky_data.csv, разделите его на train/test и обучите на нем нейронную сеть (архитектура ниже). Обучайте на батчах с помощью оптимизатора Adam, lr подберите сами, пробуйте что-то вроде 1e-2\n",
    "\n",
    "Архитектура:\n",
    "\n",
    "1. Dense Layer с relu активацией и 50 нейронами\n",
    "2. Dropout 80% (если другой keep rate дает сходимость лучше, то можно изменить) (попробуйте 50%) \n",
    "3. BatchNorm\n",
    "4. Dense Layer с relu активацией и 100 нейронами\n",
    "5. Dropout 80% (если другой keep rate дает сходимость лучше, то можно изменить) (попробуйте для разнообразия 50%)\n",
    "6. BatchNorm\n",
    "7. Выходной Dense слой c количеством нейронов, равному количеству классов\n",
    "\n",
    "Лосс - CrossEntropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTd7VFMskQMw"
   },
   "source": [
    "В датасете классы представлены строками, поэтому классы нужно закодировать. Для этого в строчке ниже объявлен dict, с помощью него и функции map превратите столбец с таргетом в целое число. Кроме того, за вас мы выделили признаки, которые нужно использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTMs6bU6kQMx"
   },
   "source": [
    "### Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci8mdz99kQMy"
   },
   "outputs": [],
   "source": [
    "feature_columns = ['ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'run', 'camcol', 'field']\n",
    "target_column = 'class'\n",
    "\n",
    "target_mapping = {\n",
    "    'GALAXY': 0,\n",
    "    'STAR': 1,\n",
    "    'QSO': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2450,
     "status": "ok",
     "timestamp": 1586246030358,
     "user": {
      "displayName": "Yury Yarovikov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip8__BUAkkFW7zB1tjXwB7Y8uEezomM5ErVG2V=s64",
      "userId": "05223355485824927663"
     },
     "user_tz": -180
    },
    "id": "QRcIYVvUkQM2",
    "outputId": "8c6b62aa-45d3-4a89-bc39-6470c22861f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GALAXY    4998\n",
       "STAR      4152\n",
       "QSO        850\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://drive.google.com/uc?id=1K-8CtATw6Sv7k2dXco1fL5MAhTbKtIH3')\n",
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1586246142286,
     "user": {
      "displayName": "Yury Yarovikov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip8__BUAkkFW7zB1tjXwB7Y8uEezomM5ErVG2V=s64",
      "userId": "05223355485824927663"
     },
     "user_tz": -180
    },
    "id": "XQJyao1zoytL",
    "outputId": "bafda877-842b-42fe-9793-c765aee737a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.722360e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.598371</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.638140e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>3.232740e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.847174</td>\n",
       "      <td>0.173694</td>\n",
       "      <td>19.43133</td>\n",
       "      <td>18.46779</td>\n",
       "      <td>18.16451</td>\n",
       "      <td>18.01475</td>\n",
       "      <td>18.04155</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.649550e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>324</td>\n",
       "      <td>51666</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.864379</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>19.38322</td>\n",
       "      <td>17.88995</td>\n",
       "      <td>17.10537</td>\n",
       "      <td>16.66393</td>\n",
       "      <td>16.36955</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.232870e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.900081</td>\n",
       "      <td>0.187473</td>\n",
       "      <td>18.97993</td>\n",
       "      <td>17.84496</td>\n",
       "      <td>17.38022</td>\n",
       "      <td>17.20673</td>\n",
       "      <td>17.07071</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.924588</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>17.90616</td>\n",
       "      <td>16.97172</td>\n",
       "      <td>16.67541</td>\n",
       "      <td>16.53776</td>\n",
       "      <td>16.47596</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>270</td>\n",
       "      <td>3.638290e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.973498</td>\n",
       "      <td>0.081626</td>\n",
       "      <td>18.67249</td>\n",
       "      <td>17.71375</td>\n",
       "      <td>17.49362</td>\n",
       "      <td>17.28284</td>\n",
       "      <td>17.22644</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>270</td>\n",
       "      <td>3.243690e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.040508</td>\n",
       "      <td>288</td>\n",
       "      <td>52000</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          objid          ra       dec         u         g         r         i  \\\n",
       "0  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1  1.237650e+18  183.598371  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "5  1.237650e+18  183.847174  0.173694  19.43133  18.46779  18.16451  18.01475   \n",
       "6  1.237650e+18  183.864379  0.019201  19.38322  17.88995  17.10537  16.66393   \n",
       "7  1.237650e+18  183.900081  0.187473  18.97993  17.84496  17.38022  17.20673   \n",
       "8  1.237650e+18  183.924588  0.097246  17.90616  16.97172  16.67541  16.53776   \n",
       "9  1.237650e+18  183.973498  0.081626  18.67249  17.71375  17.49362  17.28284   \n",
       "\n",
       "          z  run  rerun  camcol  field     specobjid   class  redshift  plate  \\\n",
       "0  15.22531  752    301       4    267  3.722360e+18    STAR -0.000009   3306   \n",
       "1  16.39150  752    301       4    267  3.638140e+17    STAR -0.000055    323   \n",
       "2  16.80125  752    301       4    268  3.232740e+17  GALAXY  0.123111    287   \n",
       "3  15.90438  752    301       4    269  3.722370e+18    STAR -0.000111   3306   \n",
       "4  16.61326  752    301       4    269  3.722370e+18    STAR  0.000590   3306   \n",
       "5  18.04155  752    301       4    269  3.649550e+17    STAR  0.000315    324   \n",
       "6  16.36955  752    301       4    269  3.232870e+17  GALAXY  0.100242    287   \n",
       "7  17.07071  752    301       4    269  3.722370e+18    STAR  0.000315   3306   \n",
       "8  16.47596  752    301       4    270  3.638290e+17    STAR  0.000089    323   \n",
       "9  17.22644  752    301       4    270  3.243690e+17  GALAXY  0.040508    288   \n",
       "\n",
       "     mjd  fiberid  \n",
       "0  54922      491  \n",
       "1  51615      541  \n",
       "2  52023      513  \n",
       "3  54922      510  \n",
       "4  54922      512  \n",
       "5  51666      594  \n",
       "6  52023      559  \n",
       "7  54922      515  \n",
       "8  51615      595  \n",
       "9  52000      400  "
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40-ivv77p9I2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ra        dec         u         g         r         i         z  \\\n",
      "0     183.531326   0.089693  19.47406  17.04240  15.94699  15.50342  15.22531   \n",
      "1     183.598371   0.135285  18.66280  17.21449  16.67637  16.48922  16.39150   \n",
      "2     183.680207   0.126185  19.38298  18.19169  17.47428  17.08732  16.80125   \n",
      "3     183.870529   0.049911  17.76536  16.60272  16.16116  15.98233  15.90438   \n",
      "4     183.883288   0.102557  17.55025  16.26342  16.43869  16.55492  16.61326   \n",
      "...          ...        ...       ...       ...       ...       ...       ...   \n",
      "9995  131.316413  51.539547  18.81777  17.47053  16.91508  16.68305  16.50570   \n",
      "9996  131.306083  51.671341  18.27255  17.43849  17.07692  16.71661  16.69897   \n",
      "9997  131.552562  51.666986  18.75818  17.77784  17.51872  17.43302  17.42048   \n",
      "9998  131.477151  51.753068  18.88287  17.91068  17.53152  17.36284  17.13988   \n",
      "9999  131.665012  51.805307  19.27586  17.37829  16.30542  15.83548  15.50588   \n",
      "\n",
      "       run  camcol  field  \n",
      "0      752       4    267  \n",
      "1      752       4    267  \n",
      "2      752       4    268  \n",
      "3      752       4    269  \n",
      "4      752       4    269  \n",
      "...    ...     ...    ...  \n",
      "9995  1345       3    161  \n",
      "9996  1345       3    162  \n",
      "9997  1345       3    162  \n",
      "9998  1345       3    163  \n",
      "9999  1345       3    163  \n",
      "\n",
      "[10000 rows x 10 columns]\n",
      "0       1\n",
      "1       1\n",
      "2       0\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "9995    0\n",
      "9996    0\n",
      "9997    1\n",
      "9998    0\n",
      "9999    0\n",
      "Name: class, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract Features\n",
    "X = data[feature_columns]\n",
    "# Extract target\n",
    "# encode target with target_mapping\n",
    "y = data[target_column].map(target_mapping)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3OkZT7HkQM7"
   },
   "source": [
    "Нормализация фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynmXS7dMkQM8"
   },
   "outputs": [],
   "source": [
    "# Просто вычтите среднее и поделитe на стандартное отклонение (с помощью пандас). Также преобразуйте всё в np.array\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "# min-max normalization\n",
    "# X = (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ra         1.786276\n",
      "dec        2.130163\n",
      "u          1.183295\n",
      "g          2.693976\n",
      "r          7.455837\n",
      "i         10.155896\n",
      "z          5.327702\n",
      "run        1.576865\n",
      "camcol     1.411189\n",
      "field      2.863983\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16745005 -0.58489347  1.0314348  -0.34854195 -0.8372384  -0.94601041\n",
      " -0.99529177 -0.83801899  0.21084118 -0.21761955]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEIewITCqo38"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Данные не отнормированы",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-510-3bbab44da409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Данные не отнормированы'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Данные не отнормированы"
     ]
    }
   ],
   "source": [
    "assert type(X) == np.ndarray and type(y) == np.ndarray, 'Проверьте, что получившиеся массивы являются np.ndarray'\n",
    "assert np.allclose(y[:5], [1,1,0,1,1])\n",
    "assert X.shape == (10000, 10)\n",
    "assert np.allclose(X.mean(axis=0), np.zeros(10)) and np.allclose(X.std(axis=0), np.ones(10)), 'Данные не отнормированы'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTcR3q0SkQNj"
   },
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5AFbCY4kQNk"
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Превратим данные в тензоры, чтобы потом было удобнее\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDCt0vtlkQNo"
   },
   "source": [
    "Хорошо, данные мы подготовили, теперь надо объявить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучайте на батчах с помощью оптимизатора Adam, lr подберите сами, пробуйте что-то вроде 1e-2\n",
    "\n",
    "Архитектура:\n",
    "\n",
    "1. Dense Layer с relu активацией и 50 нейронами\n",
    "2. Dropout 80% (если другой keep rate дает сходимость лучше, то можно изменить) (попробуйте 50%) \n",
    "3. BatchNorm\n",
    "4. Dense Layer с relu активацией и 100 нейронами\n",
    "5. Dropout 80% (если другой keep rate дает сходимость лучше, то можно изменить) (попробуйте для разнообразия 50%)\n",
    "6. BatchNorm\n",
    "7. Выходной Dense слой c количеством нейронов, равному количеству классов\n",
    "\n",
    "Лосс - CrossEntropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fI6ZqCaCkQNp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.25, inplace=False)\n",
      "  (3): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Dropout(p=0.5, inplace=False)\n",
      "  (7): Linear(in_features=100, out_features=3, bias=True)\n",
      "  (8): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) \n",
    "np.random.seed(42)\n",
    "eta = 0.01\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(50, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_features=100),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(100, 3),\n",
    "    nn.BatchNorm1d(num_features=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkUkeHfokQNs"
   },
   "source": [
    "### Обучающий цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41jYcT6AkQNt"
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, num_epoch):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(num_epoch):\n",
    "        epoch_train_losses = []\n",
    "        epoch_test_losses = []\n",
    "        #optimizer.zero_grad()\n",
    "        #avg_loss = 0.\n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, 500):\n",
    "            # На лекции мы рассказывали, что дропаут работает по-разному во время обучения и реального предсказания\n",
    "            # Чтобы это учесть нам нужно включать и выключать режим обучения, делается это командой ниже\n",
    "            model.train(True)\n",
    "            \n",
    "            # Посчитаем предсказание и лосс\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # зануляем градиент\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward\n",
    "            # ОБНОВЛЯЕМ веса\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Запишем число (не тензор) в наши батчевые лоссы\n",
    "            epoch_train_losses.append(loss.item()) \n",
    "            \n",
    "            #loss = loss / len(y_batch)\n",
    "            #avg_loss += loss.item() / len(X_train)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Теперь посчитаем лосс на тесте\n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in batch_generator(X_test, y_test, 500):\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                epoch_test_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "        test_loss = np.mean(epoch_test_losses)\n",
    "        test_losses.append(test_loss)\n",
    "        print('Epoch {}/{} \\t train_loss={:.4f} \\t test_loss={:.4f}'.format(\n",
    "        i + 1, num_epoch, train_loss, test_loss))\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idGcIKlIth3D"
   },
   "outputs": [],
   "source": [
    "def check_loss_decreased():\n",
    "    print(\"На графике сверху, точно есть сходимость? Точно-точно? [Y/N]\")\n",
    "    s = input()\n",
    "    if s.lower() == 'y':\n",
    "        print(\"Хорошо!\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Можно уменьшить дропаут, уменьшить lr, поправить архитектуру, etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDyg5zMckQOX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 \t train_loss=0.8999 \t test_loss=0.8108\n",
      "Epoch 2/100 \t train_loss=0.7328 \t test_loss=0.6704\n",
      "Epoch 3/100 \t train_loss=0.6348 \t test_loss=0.5510\n",
      "Epoch 4/100 \t train_loss=0.5129 \t test_loss=0.4156\n",
      "Epoch 5/100 \t train_loss=0.4119 \t test_loss=0.3399\n",
      "Epoch 6/100 \t train_loss=0.3501 \t test_loss=0.3059\n",
      "Epoch 7/100 \t train_loss=0.3140 \t test_loss=0.2834\n",
      "Epoch 8/100 \t train_loss=0.2893 \t test_loss=0.2589\n",
      "Epoch 9/100 \t train_loss=0.2715 \t test_loss=0.2442\n",
      "Epoch 10/100 \t train_loss=0.2568 \t test_loss=0.2344\n",
      "Epoch 11/100 \t train_loss=0.2477 \t test_loss=0.2345\n",
      "Epoch 12/100 \t train_loss=0.2383 \t test_loss=0.2132\n",
      "Epoch 13/100 \t train_loss=0.2261 \t test_loss=0.2066\n",
      "Epoch 14/100 \t train_loss=0.2266 \t test_loss=0.2182\n",
      "Epoch 15/100 \t train_loss=0.2321 \t test_loss=0.2115\n",
      "Epoch 16/100 \t train_loss=0.2199 \t test_loss=0.1969\n",
      "Epoch 17/100 \t train_loss=0.2104 \t test_loss=0.1899\n",
      "Epoch 18/100 \t train_loss=0.2099 \t test_loss=0.1904\n",
      "Epoch 19/100 \t train_loss=0.2020 \t test_loss=0.1934\n",
      "Epoch 20/100 \t train_loss=0.2001 \t test_loss=0.1790\n",
      "Epoch 21/100 \t train_loss=0.1903 \t test_loss=0.1728\n",
      "Epoch 22/100 \t train_loss=0.1893 \t test_loss=0.1779\n",
      "Epoch 23/100 \t train_loss=0.1899 \t test_loss=0.1739\n",
      "Epoch 24/100 \t train_loss=0.1876 \t test_loss=0.1815\n",
      "Epoch 25/100 \t train_loss=0.1963 \t test_loss=0.1716\n",
      "Epoch 26/100 \t train_loss=0.1876 \t test_loss=0.1662\n",
      "Epoch 27/100 \t train_loss=0.1792 \t test_loss=0.1752\n",
      "Epoch 28/100 \t train_loss=0.1859 \t test_loss=0.1652\n",
      "Epoch 29/100 \t train_loss=0.1748 \t test_loss=0.1542\n",
      "Epoch 30/100 \t train_loss=0.1719 \t test_loss=0.1572\n",
      "Epoch 31/100 \t train_loss=0.1730 \t test_loss=0.1568\n",
      "Epoch 32/100 \t train_loss=0.1764 \t test_loss=0.1648\n",
      "Epoch 33/100 \t train_loss=0.1745 \t test_loss=0.1651\n",
      "Epoch 34/100 \t train_loss=0.1719 \t test_loss=0.1550\n",
      "Epoch 35/100 \t train_loss=0.1735 \t test_loss=0.1538\n",
      "Epoch 36/100 \t train_loss=0.1677 \t test_loss=0.1462\n",
      "Epoch 37/100 \t train_loss=0.1630 \t test_loss=0.1503\n",
      "Epoch 38/100 \t train_loss=0.1617 \t test_loss=0.1427\n",
      "Epoch 39/100 \t train_loss=0.1580 \t test_loss=0.1349\n",
      "Epoch 40/100 \t train_loss=0.1586 \t test_loss=0.1470\n",
      "Epoch 41/100 \t train_loss=0.1596 \t test_loss=0.1706\n",
      "Epoch 42/100 \t train_loss=0.1712 \t test_loss=0.1461\n",
      "Epoch 43/100 \t train_loss=0.1572 \t test_loss=0.1392\n",
      "Epoch 44/100 \t train_loss=0.1531 \t test_loss=0.1468\n",
      "Epoch 45/100 \t train_loss=0.1642 \t test_loss=0.1491\n",
      "Epoch 46/100 \t train_loss=0.1720 \t test_loss=0.1377\n",
      "Epoch 47/100 \t train_loss=0.1558 \t test_loss=0.1412\n",
      "Epoch 48/100 \t train_loss=0.1558 \t test_loss=0.1379\n",
      "Epoch 49/100 \t train_loss=0.1622 \t test_loss=0.1458\n",
      "Epoch 50/100 \t train_loss=0.1604 \t test_loss=0.1368\n",
      "Epoch 51/100 \t train_loss=0.1551 \t test_loss=0.1364\n",
      "Epoch 52/100 \t train_loss=0.1487 \t test_loss=0.1453\n",
      "Epoch 53/100 \t train_loss=0.1542 \t test_loss=0.1342\n",
      "Epoch 54/100 \t train_loss=0.1462 \t test_loss=0.1311\n",
      "Epoch 55/100 \t train_loss=0.1570 \t test_loss=0.1392\n",
      "Epoch 56/100 \t train_loss=0.1485 \t test_loss=0.1318\n",
      "Epoch 57/100 \t train_loss=0.1488 \t test_loss=0.1328\n",
      "Epoch 58/100 \t train_loss=0.1430 \t test_loss=0.1330\n",
      "Epoch 59/100 \t train_loss=0.1499 \t test_loss=0.1361\n",
      "Epoch 60/100 \t train_loss=0.1539 \t test_loss=0.1318\n",
      "Epoch 61/100 \t train_loss=0.1526 \t test_loss=0.1309\n",
      "Epoch 62/100 \t train_loss=0.1488 \t test_loss=0.1345\n",
      "Epoch 63/100 \t train_loss=0.1441 \t test_loss=0.1270\n",
      "Epoch 64/100 \t train_loss=0.1486 \t test_loss=0.1284\n",
      "Epoch 65/100 \t train_loss=0.1473 \t test_loss=0.1356\n",
      "Epoch 66/100 \t train_loss=0.1485 \t test_loss=0.1284\n",
      "Epoch 67/100 \t train_loss=0.1473 \t test_loss=0.1291\n",
      "Epoch 68/100 \t train_loss=0.1458 \t test_loss=0.1304\n",
      "Epoch 69/100 \t train_loss=0.1510 \t test_loss=0.1312\n",
      "Epoch 70/100 \t train_loss=0.1474 \t test_loss=0.1436\n",
      "Epoch 71/100 \t train_loss=0.1497 \t test_loss=0.1273\n",
      "Epoch 72/100 \t train_loss=0.1374 \t test_loss=0.1260\n",
      "Epoch 73/100 \t train_loss=0.1453 \t test_loss=0.1245\n",
      "Epoch 74/100 \t train_loss=0.1390 \t test_loss=0.1305\n",
      "Epoch 75/100 \t train_loss=0.1485 \t test_loss=0.1272\n",
      "Epoch 76/100 \t train_loss=0.1409 \t test_loss=0.1292\n",
      "Epoch 77/100 \t train_loss=0.1434 \t test_loss=0.1277\n",
      "Epoch 78/100 \t train_loss=0.1435 \t test_loss=0.1286\n",
      "Epoch 79/100 \t train_loss=0.1440 \t test_loss=0.1230\n",
      "Epoch 80/100 \t train_loss=0.1467 \t test_loss=0.1266\n",
      "Epoch 81/100 \t train_loss=0.1463 \t test_loss=0.1277\n",
      "Epoch 82/100 \t train_loss=0.1429 \t test_loss=0.1302\n",
      "Epoch 83/100 \t train_loss=0.1383 \t test_loss=0.1297\n",
      "Epoch 84/100 \t train_loss=0.1421 \t test_loss=0.1263\n",
      "Epoch 85/100 \t train_loss=0.1400 \t test_loss=0.1272\n",
      "Epoch 86/100 \t train_loss=0.1441 \t test_loss=0.1246\n",
      "Epoch 87/100 \t train_loss=0.1446 \t test_loss=0.1277\n",
      "Epoch 88/100 \t train_loss=0.1365 \t test_loss=0.1226\n",
      "Epoch 89/100 \t train_loss=0.1427 \t test_loss=0.1316\n",
      "Epoch 90/100 \t train_loss=0.1384 \t test_loss=0.1264\n",
      "Epoch 91/100 \t train_loss=0.1344 \t test_loss=0.1286\n",
      "Epoch 92/100 \t train_loss=0.1398 \t test_loss=0.1254\n",
      "Epoch 93/100 \t train_loss=0.1352 \t test_loss=0.1257\n",
      "Epoch 94/100 \t train_loss=0.1373 \t test_loss=0.1205\n",
      "Epoch 95/100 \t train_loss=0.1324 \t test_loss=0.1221\n",
      "Epoch 96/100 \t train_loss=0.1326 \t test_loss=0.1240\n",
      "Epoch 97/100 \t train_loss=0.1344 \t test_loss=0.1248\n",
      "Epoch 98/100 \t train_loss=0.1375 \t test_loss=0.1287\n",
      "Epoch 99/100 \t train_loss=0.1326 \t test_loss=0.1271\n",
      "Epoch 100/100 \t train_loss=0.1369 \t test_loss=0.1217\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На графике сверху, точно есть сходимость? Точно-точно? [Y/N]\n",
      "y\n",
      "Хорошо!\n"
     ]
    }
   ],
   "source": [
    "#Подберите количество эпох так, чтобы график loss сходился\n",
    "train_losses, test_losses = train(X_train, y_train, X_test, y_test, 100) \n",
    "plt.plot(range(len(train_losses)), train_losses, label='train')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "check_loss_decreased()\n",
    "assert train_losses[-1] < 0.3 and test_losses[-1] < 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UV1jaOM1SuTL"
   },
   "source": [
    "### Вычислите accuracy получившейся модели на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXqXflGcTBKS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9690666666666666\n",
      "Test accuracy: 0.9636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "model.eval()\n",
    "train_pred_labels = torch.max(model(X_train), 1).indices\n",
    "test_pred_labels = torch.max(model(X_test), 1).indices\n",
    "\n",
    "#print(train_pred_labels)\n",
    "#print(test_pred_labels)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred_labels)\n",
    "test_acc = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "assert train_acc > 0.9, \"Если уж классифицировать звезды, которые уже видел, то не хуже, чем в 90% случаев\"\n",
    "assert test_acc > 0.9, \"Новые звезды тоже надо классифицировать хотя бы в 90% случаев\"\n",
    "\n",
    "print(\"Train accuracy: {}\\nTest accuracy: {}\".format(train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IB1XswA2kQOd"
   },
   "source": [
    "# Задание 3. Исправление ошибок в архитектуре\n",
    "\n",
    "Только что вы обучили полносвязную нейронную сеть. Теперь вам предстоит проанализировать архитектуру нейронной сети ниже, исправить в ней ошибки и  обучить её с помощью той же функции train. \n",
    "\n",
    "Будьте осторожнее и убедитесь, что перед запуском train вы вновь переопределили все необходимые внешние переменные (train обращается к глобальным переменным, в целом так делать не стоит, но сейчас это было оправдано, так как иначе нам пришлось бы передавать порядка 7-8 аргументов).\n",
    "\n",
    "Чтобы у вас получилась такая же архитектура, как у нас, и ответы совпали, давайте определим некоторые правила, как исправлять ошибки:\n",
    "\n",
    "1. Если вы видите лишний нелинейный слой, который стоит не на своем месте, просто удалите его. (не нужно добавлять новые слои, чтобы сделать постановку изначального слоя разумной. Удалять надо самый последний слой, который все портит. Для линейных слоев надо что-то исправить, а не удалить его)\n",
    "2. Если у слоя нет активации, то добавьте ReLU или другую подходящую активацию\n",
    "3. Если что-то не так с learning_rate, то поставьте 1e-2\n",
    "4. Если что-то не так с параметрами, считайте первый параметр, который появляется, как верный (т.е. далее в сети должен использоваться он).\n",
    "5. Ошибки могут быть и в полносвязных слоях. \n",
    "6. Любые другие проблемы решаются более менее однозначно, если же у вас есть серьезные сомнения, то напишите в беседу в телеграме и пинганите меня @runfme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un7PyM39kQOe"
   },
   "source": [
    "Задача все та же - классификация небесных объектов на том же датасете. После исправления сети вам нужно обучить ее.\n",
    "\n",
    "**Ответ на задачу - средний лосс на тестовом датасете**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3M9P67WekQOe"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)   \n",
    "np.random.seed(42)\n",
    "# WRONG ARCH\n",
    "model = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(6, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(100, 200),\n",
    "    nn.Softmax(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(200, 3),\n",
    "    nn.Dropout(p=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters[:-2], lr=1e-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0HEx6vbkQOi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.25, inplace=False)\n",
      "  (3): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.25, inplace=False)\n",
      "  (6): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.25, inplace=False)\n",
      "  (9): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (10): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# RIGHT ARCH\n",
    "torch.manual_seed(42)   \n",
    "np.random.seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(50, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(100, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(200, 3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGhmQg06gGiT"
   },
   "source": [
    "### Обучите и протестируйте модель так же, как вы это сделали в задаче 2. Вычислите accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(X_train, y_train, X_test, y_test, num_epoch):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(num_epoch):\n",
    "        epoch_train_losses = []\n",
    "        epoch_test_losses = []\n",
    "        #optimizer.zero_grad()\n",
    "        #avg_loss = 0.\n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, 500):\n",
    "            model.train(True)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in batch_generator(X_test, y_test, 500):\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                epoch_test_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "        test_loss = np.mean(epoch_test_losses)\n",
    "        test_losses.append(test_loss)\n",
    "        print('Epoch {}/{} \\t train_loss={:.4f} \\t test_loss={:.4f}'.format(\n",
    "        i + 1, num_epoch, train_loss, test_loss))\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 \t train_loss=0.8012 \t test_loss=0.6761\n",
      "Epoch 2/100 \t train_loss=0.6383 \t test_loss=0.6058\n",
      "Epoch 3/100 \t train_loss=0.5762 \t test_loss=0.5246\n",
      "Epoch 4/100 \t train_loss=0.4945 \t test_loss=0.3698\n",
      "Epoch 5/100 \t train_loss=0.3813 \t test_loss=0.2823\n",
      "Epoch 6/100 \t train_loss=0.3154 \t test_loss=0.2600\n",
      "Epoch 7/100 \t train_loss=0.2781 \t test_loss=0.2434\n",
      "Epoch 8/100 \t train_loss=0.2591 \t test_loss=0.2268\n",
      "Epoch 9/100 \t train_loss=0.2443 \t test_loss=0.2152\n",
      "Epoch 10/100 \t train_loss=0.2346 \t test_loss=0.2118\n",
      "Epoch 11/100 \t train_loss=0.2256 \t test_loss=0.2102\n",
      "Epoch 12/100 \t train_loss=0.2214 \t test_loss=0.1942\n",
      "Epoch 13/100 \t train_loss=0.2148 \t test_loss=0.1878\n",
      "Epoch 14/100 \t train_loss=0.2137 \t test_loss=0.1973\n",
      "Epoch 15/100 \t train_loss=0.2075 \t test_loss=0.1926\n",
      "Epoch 16/100 \t train_loss=0.2085 \t test_loss=0.1922\n",
      "Epoch 17/100 \t train_loss=0.2035 \t test_loss=0.1941\n",
      "Epoch 18/100 \t train_loss=0.2028 \t test_loss=0.1917\n",
      "Epoch 19/100 \t train_loss=0.2026 \t test_loss=0.1960\n",
      "Epoch 20/100 \t train_loss=0.1965 \t test_loss=0.1755\n",
      "Epoch 21/100 \t train_loss=0.1863 \t test_loss=0.1816\n",
      "Epoch 22/100 \t train_loss=0.1899 \t test_loss=0.1794\n",
      "Epoch 23/100 \t train_loss=0.1847 \t test_loss=0.1782\n",
      "Epoch 24/100 \t train_loss=0.1882 \t test_loss=0.1759\n",
      "Epoch 25/100 \t train_loss=0.1913 \t test_loss=0.1710\n",
      "Epoch 26/100 \t train_loss=0.1838 \t test_loss=0.1659\n",
      "Epoch 27/100 \t train_loss=0.1844 \t test_loss=0.1718\n",
      "Epoch 28/100 \t train_loss=0.1836 \t test_loss=0.1709\n",
      "Epoch 29/100 \t train_loss=0.1738 \t test_loss=0.1734\n",
      "Epoch 30/100 \t train_loss=0.1769 \t test_loss=0.1644\n",
      "Epoch 31/100 \t train_loss=0.1734 \t test_loss=0.1547\n",
      "Epoch 32/100 \t train_loss=0.1769 \t test_loss=0.1634\n",
      "Epoch 33/100 \t train_loss=0.1736 \t test_loss=0.1560\n",
      "Epoch 34/100 \t train_loss=0.1792 \t test_loss=0.1555\n",
      "Epoch 35/100 \t train_loss=0.1654 \t test_loss=0.1589\n",
      "Epoch 36/100 \t train_loss=0.1744 \t test_loss=0.1558\n",
      "Epoch 37/100 \t train_loss=0.1680 \t test_loss=0.1602\n",
      "Epoch 38/100 \t train_loss=0.1756 \t test_loss=0.1647\n",
      "Epoch 39/100 \t train_loss=0.1666 \t test_loss=0.1462\n",
      "Epoch 40/100 \t train_loss=0.1585 \t test_loss=0.1521\n",
      "Epoch 41/100 \t train_loss=0.1652 \t test_loss=0.1534\n",
      "Epoch 42/100 \t train_loss=0.1615 \t test_loss=0.1487\n",
      "Epoch 43/100 \t train_loss=0.1648 \t test_loss=0.1488\n",
      "Epoch 44/100 \t train_loss=0.1627 \t test_loss=0.1556\n",
      "Epoch 45/100 \t train_loss=0.1661 \t test_loss=0.1445\n",
      "Epoch 46/100 \t train_loss=0.1625 \t test_loss=0.1453\n",
      "Epoch 47/100 \t train_loss=0.1564 \t test_loss=0.1344\n",
      "Epoch 48/100 \t train_loss=0.1637 \t test_loss=0.1600\n",
      "Epoch 49/100 \t train_loss=0.1655 \t test_loss=0.1439\n",
      "Epoch 50/100 \t train_loss=0.1615 \t test_loss=0.1470\n",
      "Epoch 51/100 \t train_loss=0.1628 \t test_loss=0.1544\n",
      "Epoch 52/100 \t train_loss=0.1617 \t test_loss=0.1418\n",
      "Epoch 53/100 \t train_loss=0.1503 \t test_loss=0.1287\n",
      "Epoch 54/100 \t train_loss=0.1500 \t test_loss=0.1365\n",
      "Epoch 55/100 \t train_loss=0.1514 \t test_loss=0.1407\n",
      "Epoch 56/100 \t train_loss=0.1523 \t test_loss=0.1380\n",
      "Epoch 57/100 \t train_loss=0.1499 \t test_loss=0.1420\n",
      "Epoch 58/100 \t train_loss=0.1557 \t test_loss=0.1389\n",
      "Epoch 59/100 \t train_loss=0.1521 \t test_loss=0.1346\n",
      "Epoch 60/100 \t train_loss=0.1442 \t test_loss=0.1276\n",
      "Epoch 61/100 \t train_loss=0.1391 \t test_loss=0.1311\n",
      "Epoch 62/100 \t train_loss=0.1404 \t test_loss=0.1259\n",
      "Epoch 63/100 \t train_loss=0.1419 \t test_loss=0.1261\n",
      "Epoch 64/100 \t train_loss=0.1398 \t test_loss=0.1317\n",
      "Epoch 65/100 \t train_loss=0.1446 \t test_loss=0.1314\n",
      "Epoch 66/100 \t train_loss=0.1514 \t test_loss=0.1511\n",
      "Epoch 67/100 \t train_loss=0.1572 \t test_loss=0.1304\n",
      "Epoch 68/100 \t train_loss=0.1373 \t test_loss=0.1313\n",
      "Epoch 69/100 \t train_loss=0.1393 \t test_loss=0.1369\n",
      "Epoch 70/100 \t train_loss=0.1485 \t test_loss=0.1266\n",
      "Epoch 71/100 \t train_loss=0.1480 \t test_loss=0.1288\n",
      "Epoch 72/100 \t train_loss=0.1458 \t test_loss=0.1431\n",
      "Epoch 73/100 \t train_loss=0.1482 \t test_loss=0.1349\n",
      "Epoch 74/100 \t train_loss=0.1372 \t test_loss=0.1247\n",
      "Epoch 75/100 \t train_loss=0.1393 \t test_loss=0.1201\n",
      "Epoch 76/100 \t train_loss=0.1419 \t test_loss=0.1321\n",
      "Epoch 77/100 \t train_loss=0.1349 \t test_loss=0.1285\n",
      "Epoch 78/100 \t train_loss=0.1405 \t test_loss=0.1221\n",
      "Epoch 79/100 \t train_loss=0.1460 \t test_loss=0.1228\n",
      "Epoch 80/100 \t train_loss=0.1414 \t test_loss=0.1358\n",
      "Epoch 81/100 \t train_loss=0.1463 \t test_loss=0.1177\n",
      "Epoch 82/100 \t train_loss=0.1389 \t test_loss=0.1237\n",
      "Epoch 83/100 \t train_loss=0.1396 \t test_loss=0.1272\n",
      "Epoch 84/100 \t train_loss=0.1386 \t test_loss=0.1250\n",
      "Epoch 85/100 \t train_loss=0.1406 \t test_loss=0.1269\n",
      "Epoch 86/100 \t train_loss=0.1391 \t test_loss=0.1228\n",
      "Epoch 87/100 \t train_loss=0.1389 \t test_loss=0.1360\n",
      "Epoch 88/100 \t train_loss=0.1433 \t test_loss=0.1363\n",
      "Epoch 89/100 \t train_loss=0.1314 \t test_loss=0.1304\n",
      "Epoch 90/100 \t train_loss=0.1301 \t test_loss=0.1282\n",
      "Epoch 91/100 \t train_loss=0.1336 \t test_loss=0.1305\n",
      "Epoch 92/100 \t train_loss=0.1324 \t test_loss=0.1280\n",
      "Epoch 93/100 \t train_loss=0.1317 \t test_loss=0.1195\n",
      "Epoch 94/100 \t train_loss=0.1342 \t test_loss=0.1238\n",
      "Epoch 95/100 \t train_loss=0.1288 \t test_loss=0.1231\n",
      "Epoch 96/100 \t train_loss=0.1326 \t test_loss=0.1244\n",
      "Epoch 97/100 \t train_loss=0.1256 \t test_loss=0.1408\n",
      "Epoch 98/100 \t train_loss=0.1335 \t test_loss=0.1242\n",
      "Epoch 99/100 \t train_loss=0.1298 \t test_loss=0.1140\n",
      "Epoch 100/100 \t train_loss=0.1274 \t test_loss=0.1137\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses, test_losses = train2(X_train, y_train, X_test, y_test, 100) \n",
    "plt.plot(range(len(train_losses)), train_losses, label='train')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SZv9yARkQOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9726666666666667\n",
      "Test accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "model.eval()\n",
    "train_pred_labels = torch.max(model(X_train), 1).indices\n",
    "test_pred_labels = torch.max(model(X_test), 1).indices\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred_labels)\n",
    "test_acc = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "#assert train_acc > 0.9, \"Если уж классифицировать звезды, которые уже видел, то не хуже, чем в 90% случаев\"\n",
    "#assert test_acc > 0.9, \"Новые звезды тоже надо классифицировать хотя бы в 90% случаев\"\n",
    "\n",
    "print(\"Train accuracy: {}\\nTest accuracy: {}\".format(train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUGWpT3MkQOr"
   },
   "source": [
    "# Задание 4. Stack layers\n",
    "\n",
    "Давайте посмотрим, когда добавление перестает улучшать метрики. Увеличивайте блоков из слоев в сети, пока минимальный лосс на тестовом датасете за все время обучения не перестанет уменьшаться (20 эпох). \n",
    "\n",
    "Стоит помнить, что нельзя переиспользовать слои с предыдущих обучений, потому что они уже будут с подобранными весами.\n",
    "\n",
    "**Чтобы получить воспроизводимость и идентичный нашему ответ, надо объявлять все слои в порядке, в котором они применяются внутри модели. Это важно, если вы будете собирать свою модель из частей. Перед объявлением этих слоев по порядку напишите**\n",
    "> torch.manual_seed(42)   \n",
    "> np.random.seed(42)\n",
    "\n",
    "**При чем каждый раз, когда вы заново создаете модель, перезадавайте random seeds**\n",
    "\n",
    "**Опитимизатор - Adam(lr=1e-2)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZzgn9y8kQOr"
   },
   "outputs": [],
   "source": [
    "# МОДЕЛЬ ДЛЯ ПРИМЕРА, НА САМОМ ДЕЛЕ ВАМ ПРИДЕТСЯ СОЗДАВАТЬ НОВУЮ МОДЕЛЬ ДЛЯ КАЖДОГО КОЛИЧЕСТВА БЛОКОВ\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(feature_columns), 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # Начало блока, который надо вставалять много раз\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(100),\n",
    "    # Конец блока\n",
    "    nn.Linear(100, 3)\n",
    "    # Блока Softmax нет, поэтому нам нужно использовать лосс - CrossEntropyLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYUngAvSkQOw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Вы уже многое умеете, поэтому теперь код надо написать самому\n",
    "# Идея - разделить модель на части.\n",
    "# Вначале создать head часть как Sequential модель, потом в цикле создать Sequential модели, которые представляют\n",
    "# из себя блоки, потом создать tail часть тоже как Sequential, а потом объединить их в одну Sequential модель \n",
    "# вот таким кодом: nn.Sequential(header, *blocks, footer)\n",
    "# Важная идея тут состоит в том, что модели могут быть частями других моделей)\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hh5U-iUTgzxY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In4h-bM_g0Vb"
   },
   "source": [
    "## Задание 5. Сделайте выводы \n",
    "Начиная с какого количества блоков минимальный лосс за время обучения увеличивается? Почему лишнее количество блоков не помогает модели? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]neural_networks_pytorch.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
